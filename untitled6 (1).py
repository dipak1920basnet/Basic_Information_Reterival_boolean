# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfGM1xCo5M08ts4vZN0wz5AkPRlxAoOX
"""

# Installing required libraries

import os
import re
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

### Stop Words Removal,

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    tokens = re.sub(r'\W+', ' ', text.lower()).split()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

###  Vectorization Using TF-IDF

def vectorize(documents):
    vectorizer = TfidfVectorizer()
    vectors = vectorizer.fit_transform(documents)
    return vectors.toarray(), vectorizer.get_feature_names_out()

### Cosine Similarity

def calculate_similarity(vectors):
    cosine_sim = cosine_similarity(vectors)
    return cosine_sim

## Displaying Results

import seaborn as sns
import matplotlib.pyplot as plt

def plot_similarity(similarity_matrix, documents):
    sns.heatmap(similarity_matrix, xticklabels=documents, yticklabels=documents, cmap="YlGnBu")
    plt.title("Cosine Similarity Between Documents")
    plt.show()

document_texts = [
    "Document 1: The quick brown fox jumps over the lazy dog. This is a sample document for testing purposes.",
    "Document 2: AI and machine learning are transforming industries by automating tasks and providing insights.",
    "Document 3: Python is a versatile programming language used for web development, data science, and more.",
    "Document 4: The global economy is shifting toward renewable energy sources to combat climate change.",
    "Document 5: In ancient times, philosophers like Plato and Aristotle debated topics such as ethics and logic."
]

import os

directory = "/mnt/data"
if not os.path.exists(directory):
    os.makedirs(directory)

file_paths = []
for i, content in enumerate(document_texts):
    file_path = f"document_{i+1}.txt"
    with open(file_path, 'w') as file:
        file.write(content)
    file_paths.append(file_path)

file_paths

def load_specific_documents(paths):
    documents = []
    doc_names = []

    for path in paths:
        with open(path, 'r', encoding='utf-8') as file:
            content = file.read()
            documents.append(preprocess(content))
            doc_names.append(path)

    return documents, doc_names

def main():
    paths = [
        'document_1.txt',
        'document_2.txt',
        'document_3.txt',
        'document_4.txt',
        'document_5.txt'
    ]

    # Step 1: Load and preprocess the documents
    documents, doc_names = load_specific_documents(paths)

    # Step 2: Vectorize the documents
    vectors, feature_names = vectorize(documents)

    # Step 3: Calculate cosine similarity
    similarity_matrix = calculate_similarity(vectors)

    # Step 4: Display the similarity matrix
    print("Cosine Similarity Matrix:")
    print(pd.DataFrame(similarity_matrix, index=doc_names, columns=doc_names))

    # Step 5: Plot the similarity matrix
    plot_similarity(similarity_matrix, doc_names)

if __name__ == "__main__":
    main()

